# Quay Blob Storage & SHA-512 Support Plan

> **Purpose:** Add SHA-512 digest support to Quay while maintaining deduplication across all digest algorithms. This document analyzes current blob storage, identifies required changes, and proposes a design that prevents storage explosion when the same content is pushed with different hash algorithms.

---

## Table of Contents

1. [OCI Spec Alignment](#1-oci-spec-alignment)
2. [How Blob Upload Works (Current)](#2-how-blob-upload-works-current)
3. [Current Content Identity Model](#3-current-content-identity-model)
4. [The Problem: Cross-Algorithm Deduplication](#4-the-problem-cross-algorithm-deduplication)
5. [Proposed Solution: Canonical SHA256 + Digest Aliases](#5-proposed-solution-canonical-sha256--digest-aliases)
6. [Detailed Scenarios](#6-detailed-scenarios)
7. [Implementation Plan](#7-implementation-plan)
8. [Migration Strategy](#8-migration-strategy)
9. [Files Requiring Changes](#9-files-requiring-changes)

---

## 1. OCI Spec Alignment

### Algorithm Requirements

| Algorithm | OCI Requirement | Spec Text |
|-----------|-----------------|-----------|
| SHA-256 | **MUST** | "Implementations MUST implement SHA-256 digest verification" |
| SHA-512 | MAY | "Implementations MAY implement SHA-512 digest verification" |
| BLAKE3 | MAY | "Implementations MAY implement BLAKE3 digest verification" |

**Our design always computes SHA-256, satisfying the mandatory requirement.**

### Registry Can Return Different Digest

The OCI Distribution Spec explicitly permits this:

> "The `Docker-Content-Digest` header, if present on the response, returns the canonical digest of the uploaded blob **which MAY differ from the provided digest**."

> "If the digest does differ, it MAY be the case that **the hashing algorithms used do not match**."

This means our approach of internally using SHA-256 as the canonical identifier while accepting other algorithms is fully spec-compliant.

### Digest Format

```
Format: algorithm ":" encoded

SHA-256: [a-f0-9]{64}   (64 lowercase hex chars)
SHA-512: [a-f0-9]{128}  (128 lowercase hex chars)
```

---

## 2. How Blob Upload Works (Current)

### Key Concept: Clients Always Send Raw Bytes

The digest is **metadata about the content**, not a transformation of it. Clients send:
1. **Raw bytes** in the request body
2. **Expected digest** as a query parameter (their locally-computed hash)

```
PUT /v2/myrepo/blobs/uploads/abc123?digest=sha512:def456... HTTP/1.1
Content-Type: application/octet-stream
Content-Length: 104857600

<100MB of raw layer bytes - NOT hashed, just the original tar.gz data>
```

The registry independently computes the hash of received bytes and compares to the client's claim.

### Current Call Graph

```
POST /v2/{repo}/blobs/uploads/          [endpoints/v2/blob.py:256]
  ├─> Check mount request (from=, mount=)
  │   └─> _try_to_mount_blob() [blob.py:163]
  │       └─> registry_model.mount_blob_into_repository()
  │           └─> model.blob.temp_link_blob() - creates UploadedBlob link
  ├─> storage.initiate_chunked_upload() → (uuid, metadata)
  └─> registry_model.create_blob_upload() - creates BlobUpload record

PATCH /v2/{repo}/blobs/uploads/{uuid}   [endpoints/v2/blob.py:359]
  └─> blob_uploader.upload_chunk() [blobuploader.py:168]
      ├─> Wrap input stream with SHA256 handler [line 207]
      ├─> storage.stream_upload_chunk() - write chunk to backend
      └─> registry_model.update_blob_upload() - persist sha_state, byte_count

PUT /v2/{repo}/blobs/uploads/{uuid}?digest=...  [endpoints/v2/blob.py:402]
  └─> blob_uploader.commit_to_blob() [blobuploader.py:290]
      ├─> _validate_digest() - compare computed SHA256 vs expected
      ├─> _finalize_blob_storage()
      │   ├─> storage.exists() - check if blob already in storage
      │   ├─> IF EXISTS: cancel upload (dedupe)
      │   └─> IF NEW: complete upload to final path
      └─> registry_model.commit_blob_upload()
          └─> store_blob_record_and_temp_link_in_repo()
              ├─> ImageStorage.get(content_checksum=digest) - find existing
              ├─> OR ImageStorage.create(...) - new blob record
              └─> UploadedBlob.create() - temp link with expiration
```

### Key Files

| File | Purpose |
|------|---------|
| `endpoints/v2/blob.py` | HTTP handlers for blob upload API |
| `data/registry_model/blobuploader.py` | Chunk handling, digest validation |
| `data/model/blob.py` | DB operations for ImageStorage, BlobUpload |
| `storage/basestorage.py` | Abstract storage interface |
| `digest/digest_tools.py` | Digest parsing, path generation |

---

## 3. Current Content Identity Model

### Database Schema

```python
# data/database.py:1228-1234
class ImageStorage(BaseModel):
    uuid = CharField(default=uuid_generator, index=True, unique=True)
    image_size = BigIntegerField(null=True)
    uncompressed_size = BigIntegerField(null=True)
    uploading = BooleanField(default=True, null=True)
    cas_path = BooleanField(default=True)
    content_checksum = CharField(null=True, index=True)  # e.g., "sha256:abc123..."
```

**Key observations:**
- `content_checksum` stores the full digest string including algorithm prefix
- Indexed but NOT unique (application-level deduplication)
- Lookup is exact string match

### Storage Path Construction

```python
# digest/digest_tools.py:50-63
def content_path(digest):
    parsed = Digest.parse_digest(digest)
    prefix = parsed.hash_bytes[0:2].zfill(2)
    return os.path.join(parsed.hash_alg, prefix, parsed.hash_bytes)

# Examples:
# sha256:abc123... → sha256/ab/abc123...
# sha512:def456... → sha512/de/def456...
```

### Where SHA-256 is Hardcoded

| Location | What's Hardcoded |
|----------|------------------|
| `data/database.py:1598` | `sha_state = ResumableSHA256Field(...)` |
| `data/fields.py:50` | `ResumableSHA256Field` class |
| `digest/digest_tools.py:66-89` | `sha256_digest()`, `sha256_digest_from_hashlib()` |
| `blobuploader.py:207,307,324` | Uses `sha_state` (SHA256 only) |
| `image/docker/schema2/__init__.py:27` | `EMPTY_LAYER_BLOB_DIGEST` constant |

---

## 4. The Problem: Cross-Algorithm Deduplication

### Current Behavior: Storage Explosion

If identical 100MB content is pushed twice with different algorithms:

```
Push 1 (sha256):
  Client: PUT ?digest=sha256:abc123...
  Registry computes: sha256 → abc123... ✓
  Store at: sha256/ab/abc123... (100MB)
  DB: ImageStorage(content_checksum="sha256:abc123...")

Push 2 (sha512, same bytes):
  Client: PUT ?digest=sha512:def456...
  Registry computes: sha256 → abc123... (but can't validate sha512!)

  If we add sha512 support naively:
  Registry computes: sha512 → def456... ✓
  Lookup: ImageStorage.get(content_checksum="sha512:def456...") → NOT FOUND
  Store at: sha512/de/def456... (another 100MB!)
  DB: ImageStorage(content_checksum="sha512:def456...")

Result: 200MB stored for identical content
```

### Why This Happens

1. `content_checksum` lookup is exact string match
2. `"sha256:abc123..."` != `"sha512:def456..."` even for same bytes
3. Storage paths differ: `sha256/ab/...` vs `sha512/de/...`
4. No mechanism to recognize identical content across algorithms

---

## 5. Proposed Solution: Canonical SHA256 + Digest Aliases

### Core Principle

**Always compute SHA-256 from raw bytes** as the canonical identifier for deduplication and storage path. The client's requested algorithm is computed for validation and stored as an alias.

```
Raw bytes (100MB) ──┬──> hashlib.sha256() ──> sha256:abc123... (CANONICAL)
                    │
                    └──> hashlib.new(client_alg) ──> sha512:def456... (for validation)
```

Both hashes are computed from the **same raw bytes** in parallel. We are NOT hashing one digest through another algorithm.

### Two Hashes Per Upload

For any upload, compute exactly two hashes:

1. **SHA-256** (always) - canonical identity, deduplication key, storage path
2. **Client's algorithm** (whatever they requested) - for validation, stored as alias

This scales to any number of algorithms without pre-computing them all.

### New Database Schema

```sql
-- Modified ImageStorage
ALTER TABLE imagestorage ADD COLUMN canonical_sha256 VARCHAR(71);
-- Format: "sha256:{64 hex chars}"
-- Used for: deduplication lookup, storage path computation

-- New DigestAlias table
CREATE TABLE digest_alias (
    id SERIAL PRIMARY KEY,
    algorithm VARCHAR(32) NOT NULL,       -- 'sha256', 'sha512', 'sha384', etc.
    digest_hex VARCHAR(128) NOT NULL,     -- hex portion only (no algorithm prefix)
    storage_id INTEGER NOT NULL REFERENCES imagestorage(id),
    created_at TIMESTAMP DEFAULT NOW(),
    UNIQUE(algorithm, digest_hex)
);

CREATE INDEX idx_digest_alias_lookup ON digest_alias(algorithm, digest_hex);
```

### Data Model Visualization

```
┌─────────────────────────────────────────────────────────────────┐
│                      DigestAlias                                 │
│  (lookup index - find blobs by ANY digest algorithm)            │
├────────────┬─────────────────────────────┬──────────────────────┤
│ algorithm  │ digest_hex                  │ storage_id (FK)      │
├────────────┼─────────────────────────────┼──────────────────────┤
│ sha256     │ abc123...                   │ 42                   │
│ sha512     │ def456...                   │ 42  ← same blob!     │
│ sha384     │ ghi789...                   │ 42  ← same blob!     │
│ sha256     │ qqq111...                   │ 43                   │
│ sha512     │ xyz999...                   │ 43                   │
└────────────┴─────────────────────────────┴──────────────────────┘
                                                   │
                                                   ▼
┌─────────────────────────────────────────────────────────────────┐
│                      ImageStorage                                │
├──────┬──────────────────────────────┬───────────────────────────┤
│ id   │ canonical_sha256             │ image_size               │
├──────┼──────────────────────────────┼───────────────────────────┤
│ 42   │ sha256:abc123...             │ 104857600                │
│ 43   │ sha256:qqq111...             │ 52428800                 │
└──────┴──────────────────────────────┴───────────────────────────┘
                   │
                   │ (storage path derived from canonical_sha256)
                   ▼
┌─────────────────────────────────────────────────────────────────┐
│                   Physical Storage                               │
│  (all blobs stored under sha256 paths, regardless of push algo) │
├─────────────────────────────────────────────────────────────────┤
│ sha256/ab/abc123...  (100MB file)                               │
│ sha256/qq/qqq111...  (50MB file)                                │
└─────────────────────────────────────────────────────────────────┘
```

---

## 6. Detailed Scenarios

### Scenario 1: First Push with SHA-256 (Common Case Today)

```
Client: PUT ?digest=sha256:abc123...
Body: <100MB raw bytes>

Registry:
  1. Compute sha256(bytes) → sha256:abc123...  (canonical)
  2. Compute sha256(bytes) → sha256:abc123...  (client's = same)
  3. Validate: computed == expected ✓
  4. Lookup: canonical_sha256="sha256:abc123..." → NOT FOUND
  5. Store bytes at: sha256/ab/abc123...
  6. Create ImageStorage(id=42, canonical_sha256="sha256:abc123...")
  7. Create DigestAlias(algorithm="sha256", digest_hex="abc123...", storage_id=42)
  8. Return 201 Created, Docker-Content-Digest: sha256:abc123...

State after:
  - Storage: sha256/ab/abc123... (100MB)
  - ImageStorage: id=42, canonical_sha256="sha256:abc123..."
  - DigestAlias: [(sha256, abc123...) → 42]
```

### Scenario 2: Same Content Pushed with SHA-512

```
Client: PUT ?digest=sha512:def456...
Body: <100MB raw bytes - identical to Scenario 1>

Registry:
  1. Compute sha256(bytes) → sha256:abc123...  (canonical)
  2. Compute sha512(bytes) → sha512:def456...  (client's)
  3. Validate: computed sha512 == expected sha512 ✓
  4. Lookup: canonical_sha256="sha256:abc123..." → FOUND (id=42)
  5. DEDUPE: Don't store bytes (already exist at sha256/ab/abc123...)
  6. Create DigestAlias(algorithm="sha512", digest_hex="def456...", storage_id=42)
  7. Return 201 Created, Docker-Content-Digest: sha512:def456...

State after:
  - Storage: sha256/ab/abc123... (still 100MB, no duplicate)
  - ImageStorage: id=42 (unchanged)
  - DigestAlias: [(sha256, abc123...) → 42, (sha512, def456...) → 42]
```

### Scenario 3: Future Algorithm (SHA-384) Added Later

```
Year 3: SHA-384 support added to Quay
Client pushes same content that was uploaded in Year 1

Client: PUT ?digest=sha384:ghi789...
Body: <100MB raw bytes - same as before>

Registry:
  1. Compute sha256(bytes) → sha256:abc123...  (canonical)
  2. Compute sha384(bytes) → sha384:ghi789...  (client's)
  3. Validate: computed sha384 == expected sha384 ✓
  4. Lookup: canonical_sha256="sha256:abc123..." → FOUND (id=42)
  5. DEDUPE: Don't store bytes
  6. Create DigestAlias(algorithm="sha384", digest_hex="ghi789...", storage_id=42)
  7. Return 201 Created, Docker-Content-Digest: sha384:ghi789...

State after:
  - Storage: sha256/ab/abc123... (still 100MB)
  - DigestAlias: [..., (sha384, ghi789...) → 42]
```

**Key insight:** We didn't need to pre-compute SHA-384 when the blob was first uploaded. The first push with any new algorithm creates the alias on demand.

### Scenario 4: Retrieval by Any Algorithm

```
Client: GET /v2/myrepo/blobs/sha512:def456...

Registry:
  1. Parse digest: algorithm="sha512", hex="def456..."
  2. Query: SELECT storage_id FROM digest_alias
            WHERE algorithm='sha512' AND digest_hex='def456...'
     Result: storage_id = 42
  3. Query: SELECT canonical_sha256 FROM imagestorage WHERE id=42
     Result: canonical_sha256 = "sha256:abc123..."
  4. Compute path: content_path("sha256:abc123...") → "sha256/ab/abc123..."
  5. Read bytes from: sha256/ab/abc123...
  6. Return bytes with header: Docker-Content-Digest: sha512:def456...
```

### Scenario 5: Cross-Repository Mount

```
Client: POST /v2/newrepo/blobs/uploads/?mount=sha512:def456...&from=oldrepo

Registry:
  1. Verify READ permission on oldrepo ✓
  2. Query: SELECT storage_id FROM digest_alias
            WHERE algorithm='sha512' AND digest_hex='def456...'
     Result: storage_id = 42
  3. Verify blob exists in oldrepo (via ManifestBlob or UploadedBlob)
  4. Create UploadedBlob linking newrepo → ImageStorage(id=42)
  5. Return 201 Created, Docker-Content-Digest: sha512:def456...

No bytes transferred, no new storage used.
```

---

## 7. Implementation Plan

### Phase 1: Database Schema Changes

```python
# data/database.py - Add to ImageStorage
class ImageStorage(BaseModel):
    # ... existing fields ...
    canonical_sha256 = CharField(null=True, index=True, unique=True)

# data/database.py - New model
class DigestAlias(BaseModel):
    algorithm = CharField()
    digest_hex = CharField()
    storage = ForeignKeyField(ImageStorage, backref='aliases')
    created_at = DateTimeField(default=datetime.now)

    class Meta:
        indexes = (
            (('algorithm', 'digest_hex'), True),  # Unique composite
        )
```

### Phase 2: Add Resumable Hash Fields

```python
# data/fields.py - Add SHA512 field
class ResumableSHA512Field(_ResumableSHAField):
    pass

# data/database.py - Update BlobUpload
class BlobUpload(BaseModel):
    # ... existing fields ...
    sha_state = ResumableSHA256Field(null=True, default=rehash.sha256)  # Keep for canonical
    client_hash_state = TextField(null=True)  # Pickled state for client's algorithm
    client_hash_algorithm = CharField(null=True)  # e.g., "sha512"
```

### Phase 3: Update Digest Tools

```python
# digest/digest_tools.py - Add generic function
def digest_from_hashlib(algorithm, hash_obj):
    """Generic digest string from any hashlib object."""
    return f"{algorithm}:{hash_obj.hexdigest()}"

def sha512_digest_from_hashlib(sha512_hash_obj):
    return f"sha512:{sha512_hash_obj.hexdigest()}"
```

### Phase 4: Update Blob Uploader

```python
# data/registry_model/blobuploader.py

def upload_chunk(self, app_config, input_fp, start_offset, length, client_digest=None):
    # Parse client's expected algorithm
    if client_digest:
        parsed = Digest.parse_digest(client_digest)
        client_algorithm = parsed.hash_alg
    else:
        client_algorithm = "sha256"  # Default

    # Always hash with SHA256 (canonical)
    input_fp = wrap_with_handler(input_fp, self.blob_upload.sha_state.update)

    # Also hash with client's algorithm if different
    if client_algorithm != "sha256":
        client_hasher = hashlib.new(client_algorithm)
        input_fp = wrap_with_handler(input_fp, client_hasher.update)
        # Store client hasher state for resumable uploads
        self.blob_upload.client_hash_state = pickle.dumps(client_hasher)
        self.blob_upload.client_hash_algorithm = client_algorithm

    # ... rest of upload logic ...

def commit_to_blob(self, app_config, expected_digest):
    parsed = Digest.parse_digest(expected_digest)

    # Compute canonical sha256
    canonical = sha256_digest_from_hashlib(self.blob_upload.sha_state)

    # Validate client's digest
    if parsed.hash_alg == "sha256":
        computed_client = canonical
    else:
        client_hasher = pickle.loads(self.blob_upload.client_hash_state)
        computed_client = digest_from_hashlib(parsed.hash_alg, client_hasher)

    if computed_client != expected_digest:
        raise BlobDigestMismatchException()

    # Dedupe by canonical sha256
    existing = ImageStorage.get_or_none(canonical_sha256=canonical)

    if existing:
        # Content already exists - don't store again
        self.storage.cancel_chunked_upload(...)
        storage_record = existing
    else:
        # New content - store at canonical path
        final_path = content_path(canonical)
        self.storage.complete_chunked_upload(..., final_path)
        storage_record = ImageStorage.create(canonical_sha256=canonical, ...)

    # Create alias for client's digest (and canonical if different)
    DigestAlias.get_or_create(
        algorithm=parsed.hash_alg,
        digest_hex=parsed.hash_bytes,
        storage=storage_record
    )
    if parsed.hash_alg != "sha256":
        canonical_parsed = Digest.parse_digest(canonical)
        DigestAlias.get_or_create(
            algorithm="sha256",
            digest_hex=canonical_parsed.hash_bytes,
            storage=storage_record
        )

    return storage_record
```

### Phase 5: Update Blob Retrieval

```python
# data/model/oci/blob.py

def get_repository_blob_by_digest(repository, blob_digest):
    """Find blob by any supported digest algorithm."""
    parsed = Digest.parse_digest(blob_digest)

    # Lookup via alias table
    try:
        alias = DigestAlias.get(
            algorithm=parsed.hash_alg,
            digest_hex=parsed.hash_bytes
        )
        storage = ImageStorage.get(id=alias.storage_id)

        # Verify blob is accessible from this repository
        # (via UploadedBlob or ManifestBlob)
        if _blob_in_repository(repository, storage):
            return storage
        return None
    except DigestAlias.DoesNotExist:
        return None
```

---

## 8. Migration Strategy

### For Existing Blobs (All SHA-256)

Existing blobs remain unchanged in storage. Migration adds metadata only.

```python
# Migration script
def migrate_existing_blobs():
    for storage in ImageStorage.select().where(ImageStorage.content_checksum.is_null(False)):
        parsed = Digest.parse_digest(storage.content_checksum)

        if parsed.hash_alg == "sha256":
            # Set canonical field
            storage.canonical_sha256 = storage.content_checksum
            storage.save()

            # Create alias record
            DigestAlias.get_or_create(
                algorithm="sha256",
                digest_hex=parsed.hash_bytes,
                storage=storage
            )
```

**Impact on existing blobs:**
- Storage location: **No change** (files stay at `sha256/ab/abc...`)
- DB records: Add `canonical_sha256` column, populate from `content_checksum`
- New index: Create `DigestAlias` entries for existing SHA-256 digests
- Lookup: Change from `content_checksum` match to alias table lookup

---

## 9. Files Requiring Changes

### Must Change

| File | Changes |
|------|---------|
| `data/database.py` | Add `canonical_sha256` to ImageStorage, add `DigestAlias` model, add `client_hash_state`/`client_hash_algorithm` to BlobUpload |
| `data/fields.py` | Add `ResumableSHA512Field` (and others as needed) |
| `digest/digest_tools.py` | Add `digest_from_hashlib()`, `sha512_digest_from_hashlib()` |
| `data/registry_model/blobuploader.py` | Dual hashing in `upload_chunk()`, canonical lookup in `commit_to_blob()` |
| `data/model/blob.py` | Update `store_blob_record_and_temp_link_in_repo()` to use canonical_sha256 |
| `data/model/oci/blob.py` | Update `get_repository_blob_by_digest()` to use DigestAlias |

### May Need Updates

| File | Potential Changes |
|------|-------------------|
| `endpoints/v2/blob.py` | Pass client digest algorithm through upload flow |
| `data/registry_model/registry_oci_model.py` | Update mount logic to use alias lookup |
| `workers/gc/` | Update GC to count references via DigestAlias |
| `image/docker/schema2/__init__.py` | Keep EMPTY_LAYER_BLOB_DIGEST as sha256 (canonical) |

### No Changes Needed

| Component | Why |
|-----------|-----|
| `DIGEST_PATTERN` regex | Already accepts any algorithm |
| `Digest.parse_digest()` | Generic parsing |
| `content_path()` | Works with any algorithm (we'll always pass sha256) |
| API routes | Use generic DIGEST_PATTERN |
| Storage backends | Path is just a string, algorithm-agnostic |

---

## Summary

| Aspect | Current | Proposed |
|--------|---------|----------|
| **Identity key** | `content_checksum` (exact match) | `canonical_sha256` + `DigestAlias` lookup |
| **Storage path** | Derived from client's digest | Always derived from SHA-256 |
| **Deduplication** | Only within same algorithm | Across all algorithms |
| **New algorithm support** | Requires pre-computation | On-demand alias creation |
| **Existing blobs** | N/A | Backfill canonical_sha256, create aliases |
| **Spec compliance** | SHA-256 only | SHA-256 (MUST) + SHA-512/others (MAY) |
